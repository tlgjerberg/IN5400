{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a CNN for CIFAR-10\n",
    "\n",
    "In this exercise, you are asked to set up a network with convolutional layers, pooling layers, and fully connected layers.\n",
    "\n",
    "\n",
    "Training on CPU for this model can take 15-30 minutes for CIFAR-10 and up to 20-30 epochs.\n",
    "GPU use for all students in IN 5400 is not yet ready, so unless you have access to own GPU, run on CPU with the default configutation below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from utils.utility_functions import datasetFashionMNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.datasets\n",
    "import torch.utils\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "#from graphviz import Digraph\n",
    "#matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-images\n",
    "\n",
    "You might need to change the data directory if you are not on a linux computer at IFI. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The output of torchvision datasets are PILImage images of range [0, 1]. \n",
    "#We transform them to Tensors of normalized range [-1, 1].\n",
    "# Data\n",
    "transform_train = transforms.Compose([\n",
    "    # The crop is to normalize image size to 32x32\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    #The crop is to normalize image size to 32\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "CIFAR_classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "## TODO: if running on the GPU servers, no common data directory is yet available. Change root to ./data and download \n",
    "## temporally to your working area on the servers. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='CIFAR10', train=True, download=True, transform=transform_train)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='CIFAR10', train=False, download=True, transform=transform_val)\n",
    "classes = CIFAR_classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "batch_size=128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect some random images \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "nofdisplay = 8\n",
    "train_loader_some = torch.utils.data.DataLoader(train_dataset, batch_size=nofdisplay, shuffle=True)\n",
    "dataiter = iter(train_loader_some)\n",
    "images, labels = dataiter.next()\n",
    "images = images*0.2 + 0.5 #multiply by 0.2 and add 0.5 to approximately reverse the Normalize transform\n",
    "\n",
    "# show images\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = npimg.transpose((1,2,0))\n",
    "    plt.imshow(npimg, interpolation='nearest')\n",
    "\n",
    "# print(torchvision.utils.make_grid(images))\n",
    "show(torchvision.utils.make_grid(images, padding=2, normalize=True, range=(0, 1)))\n",
    "\n",
    "# print labels\n",
    "\n",
    "print(' '.join('%5s' % CIFAR_classes[labels[j]] for j in range(nofdisplay)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of important parameters, we use dictionary \"config\". You should experiment with different values for the batch size, learning rate and number of epochs trained.\n",
    "\n",
    "\n",
    "Once your implementation work, you can try to improve the accuracy by changing the parameters, and the architecture.\n",
    "\n",
    "When your code works with e.g. a couple of epochs, try using the GPU servers to do a longer traning and play with parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          'batch_size':128,\n",
    "          'use_cuda': True,       #True=use Nvidia GPU | False use CPU\n",
    "          'log_interval':20,      #How often to dislay (batch) loss during training\n",
    "          'epochs': 10,           #Number of epochs\n",
    "          'learningRate': 0.01,\n",
    "          'momentum' : 0.9\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up initialization and the basic network architecture.\n",
    "\n",
    "Set up a basic network with 2 convolutional layers, each followed by max pool, uses Relu activation, and has two fully connected layers at the end. For RGB color images, the input layer has 3 band, and for gray level images it only has 1 input band.\n",
    "\n",
    "Suggestions: Conv-layer 1 can have 6 5x5-filters, ReLU, followed by max pool with size 2.\n",
    "\n",
    "Conv-layer 2 can initially have 16 5x5-filters, ReLU, followed by maxpool with size 2.\n",
    "\n",
    "Then reshape the input to match a linear layer. Use initially 3 fully conntectedlayers, with output size 120, 84, and 10.\n",
    "\n",
    "The number of output nodes in the output layer must correspond to the number of classes, in our case 10.\n",
    "\n",
    "The resulting network will resemble LeNET, the first pioneering convolutional network, but with ReLU. \n",
    "\n",
    "Hint: see the solution to the exercise in week 5. Use nn.Conv2D, nn. MaxPool2d, and nn.Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        #Start with the solution to the first pytorch exercise. \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Let x propage from input to output.\n",
    "        # Take care by reshaping using x.view when flattening the data from convolutional to fully connected layers.\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat an instance of Model\n",
    "model = Model()\n",
    "if config['use_cuda'] == True:\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define optimizer and loss function\n",
    "\n",
    "Instantiate an optimizer, e.g. stochastic gradient descent, from the \"torch.optim\" module (https://pytorch.org/docs/stable/optim.html) with your model. Remember that we have defined \"learning rate\" inside the config-dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of \"torch.optim.SGD\"\n",
    "import torch.optim as optim\n",
    "\n",
    "#ToDo\n",
    "# Try with and without momentum, and with Adam\n",
    "optimizer = optim.SGD(model.parameters(), lr=config['learningRate'])\n",
    "#optimizer = optim.Adam(model.parameters(), lr=config['learningRate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to define the loss function (often called criterion). As we are dealing with a classification problem, you should use the softmax cross entropy loss.\n",
    "\n",
    "Hint, have a look here: (https://pytorch.org/docs/stable/nn.html#torch-nn-functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(prediction, labels):\n",
    "    \"\"\"Returns softmax cross entropy loss.\"\"\"\n",
    "    #ToDo\n",
    "    import torch.nn.functional as F\n",
    "    loss = F.cross_entropy(input=prediction, target=labels)\n",
    "   \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up the training process and train the model\n",
    "\n",
    "You have all the building blocks needed to set up the training process. You will implement the function \"run_epoch\" which shall loop though a dataset and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, epoch, data_loader, optimizer, is_training, config):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model        (obj): The neural network model\n",
    "        epoch        (int): The current epoch\n",
    "        data_loader  (obj): A pytorch data loader \"torch.utils.data.DataLoader\"\n",
    "        optimizer    (obj): A pytorch optimizer \"torch.optim\"\n",
    "        is_training (bool): Whether to use train (update) the model/weights or not. \n",
    "        config      (dict): Configuration parameters\n",
    "\n",
    "    Intermediate:\n",
    "        totalLoss: (float): The accumulated loss from all batches. \n",
    "                            Hint: Should be a numpy scalar and not a pytorch scalar\n",
    "\n",
    "    Returns:\n",
    "        loss_avg         (float): The average loss of the dataset\n",
    "        accuracy         (float): The average accuracy of the dataset\n",
    "        confusion_matrix (float): A 10x10 matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_training==True: \n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss        = 0 \n",
    "    correct          = 0 \n",
    "    confusion_matrix = np.zeros(shape=(10,10))\n",
    "    labels_list      = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "        if config['use_cuda'] == True:\n",
    "            images = data_batch[0].to('cuda') # send data to GPU\n",
    "            labels = data_batch[1].to('cuda') # send data to GPU\n",
    "        else:\n",
    "            images = data_batch[0]\n",
    "            labels = data_batch[1]\n",
    "\n",
    "        if not is_training:\n",
    "            with torch.no_grad():\n",
    "                # ToDo: Forward\n",
    "                prediction = model.forward(images)\n",
    "                \n",
    "                \n",
    "                # ToDo: Compute loss\n",
    "                # Note: It can be beneficial to detach \"total_loss\" from the graph, consider convert \"total_loss\" to numpy.\n",
    "                loss = loss_fn(prediction, labels)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "        elif is_training: \n",
    "            \n",
    "            \n",
    "            # ToDo: Forward\n",
    "            prediction = model.forward(images)\n",
    "          \n",
    "            # ToDo: Compute loss\n",
    "            # Note: It can be beneficial to detach \"total_loss\" from the graph, consider convert \"total_loss\" to numpy.\n",
    "            loss = loss_fn(prediction, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # ToDo: take a gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        # Compute the correct classification\n",
    "        predicted_label  = prediction.max(1, keepdim=True)[1][:,0]\n",
    "        correct          += predicted_label.eq(labels).cpu().sum().numpy()\n",
    "        confusion_matrix += metrics.confusion_matrix(labels.cpu().numpy(), predicted_label.cpu().numpy(), labels_list)\n",
    "\n",
    "        # Print statistics\n",
    "        batchSize = len(labels)\n",
    "        if batch_idx % config['log_interval'] == 0:\n",
    "            print(f'Epoch={epoch} | {batch_idx/len(data_loader)*100:.2f}% | loss = {loss/batchSize:.5f}')\n",
    "\n",
    "    loss_avg         = total_loss / len(data_loader)\n",
    "    accuracy         = correct / len(data_loader.dataset)\n",
    "    if  is_training:\n",
    "        #print(correct)\n",
    "        #print(len(data_loader.dataset))\n",
    "        print('Current training accuracy:', accuracy)\n",
    "    if not is_training:\n",
    "    #    print(correct)\n",
    "    #    print(len(data_loader.dataset))\n",
    "        print('Current validation accuracy',accuracy)\n",
    "    confusion_matrix = confusion_matrix / len(data_loader.dataset)\n",
    "\n",
    "    return loss_avg, accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "train_loss = np.zeros(shape=config['epochs'])\n",
    "train_acc  = np.zeros(shape=config['epochs'])\n",
    "val_loss   = np.zeros(shape=config['epochs'])\n",
    "val_acc    = np.zeros(shape=config['epochs'])\n",
    "val_confusion_matrix   = np.zeros(shape=(10,10,config['epochs']))\n",
    "train_confusion_matrix = np.zeros(shape=(10,10,config['epochs']))\n",
    "init_lr = config['learningRate']\n",
    "for epoch in range(config['epochs']):\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss[epoch], train_acc[epoch], train_confusion_matrix[:,:,epoch] = \\\n",
    "                               run_epoch(model, epoch, train_loader, optimizer, is_training=True, config=config)\n",
    "\n",
    "    val_loss[epoch], val_acc[epoch], val_confusion_matrix[:,:,epoch]     = \\\n",
    "                               run_epoch(model, epoch, val_loader, optimizer, is_training=False, config=config)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training accuracy and the training loss\n",
    "#plt.figure()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "# plt.subplots_adjust(hspace=2)\n",
    "ax.plot(train_loss, 'b', label='train loss')\n",
    "ax.plot(val_loss, 'r', label='validation loss')\n",
    "ax.grid()\n",
    "plt.ylabel(' Loss', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "ax.legend(loc='upper right', fontsize=16)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "ax.plot(train_acc, 'b', label='train_acc')\n",
    "ax.plot(val_acc, 'r', label='validation accuracy')\n",
    "ax.grid()\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "val_acc_max = np.max(val_acc)\n",
    "val_acc_max_ind = np.argmax(val_acc)\n",
    "\n",
    "\n",
    "\n",
    "plt.axvline(x=val_acc_max_ind, color='g', linestyle='--', label='Highest validation accuracy')\n",
    "plt.title('Highest validation accuracy = %0.1f %%' % (val_acc_max*100), fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(val_acc)\n",
    "class_accuracy = val_confusion_matrix[:,:,ind]\n",
    "for ii in range(len(classes)):\n",
    "    acc = val_confusion_matrix[ii,ii,ind] / np.sum(val_confusion_matrix[ii,:,ind])\n",
    "    print(f'Accuracy of {str(classes[ii]).ljust(15)}: {acc*100:.01f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve\n",
    "\n",
    "Now we have trained a basic network. Try using different learning rates, e.g. from 0.1, 0.01, 0.001 and smaller.\n",
    "\n",
    "How does the learning loss function and the accuracy for the traning set look? \n",
    "\n",
    "What does it mean when the accuracy of the traning data set is much lower than on the traning data set?\n",
    "\n",
    "\n",
    "Note that we have no regularization, so the generalization ability of the network is likely to be limited. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work\n",
    "\n",
    "Try to change the architecture, optimizer and the learning rate. Add more layers and more filters. Add batchnorm.  Does the performance improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
